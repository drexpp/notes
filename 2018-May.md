May 2018
==========

Tech
----


### Machine learning

 ![alt text](https://www.robinwieruch.de/img/posts/machine-learning-javascript-web-developers/machine-learning-hierarchy.jpg)

  - In unsupervised learning the agent learns patterns in the input even though no explicit feedback is supplied. The most common unsupervised learning task is clustering: detecting potentially useful clusters of input examples.
  - In reinforcement learning the agent learns from a series of reinforcements—rewards or punishments.
  - In supervised learning the agent observes some example input–output pairs and learns a function that maps from input to output. (You are already giving to the algorithm the "right" data, and we want to make predicitions out of it) (**Classification/forecasting or predictions**, in the example of predictions in an simple chase the value of exe Y is discrete, meaning it can only take certain values, rather than continuous).
  
![alt text](https://i.imgur.com/DYoO1Zg.png)

  (The learning agent can theoretically maximize its expected utility by choosing the hypothesis
that minimizes expected loss over all input–output pairs it will see)

 - **LINEAR REGRESSION** 
 
 (**video**)
 
 [![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/5u4G23_OohI/0.jpg)](https://www.youtube.com/watch?v=5u4G23_OohI)
 
 <ins>**Hypothesis, Cost Function and Goal**</ins>
 
 
![alt text](https://billyinn.files.wordpress.com/2014/07/e5b18fe5b995e5bfabe785a7-2014-07-12-e4b88be58d8812-53-04.png?w=625&h=383)

In order to minimize J(Theta) we will use a mathematical method called "**Gradient descent**"

 - **Batch Gradient Descent (Reduced training sets)**
 
 ![alt text](https://billyinn.files.wordpress.com/2014/07/e5b18fe5b995e5bfabe785a7-2014-07-12-e4b88be58d8812-53-35.png?w=768&h=220)
 
 Where the partial derivative of J(Theta) is: **Remember chain rule**
 
 ![alt text](https://i.gyazo.com/c160300368b198a31c0f696afae7d1d3.png)
 
 Derivative of a function gives you the direction of the steepest descent.
 
 For convergence you can use different heuristics when they change of the J(Theta) is small or any other.
 
  - **Stochastic Gradient Descent (Big training sets)**
  
  ![alt text](https://i.gyazo.com/ec242bf88b986e1c7a93ffe96dcf6f7c.png)
  
  
  
  
### Java 8 (curiosidad)
```
List<Integer> numbers = Arrays.asList(1, 2, 3, 4, 5, 6);

//Normal:
for(int i = 0; i < numbers.size(); i++){
    System.out.println(numbers.get(i));
}

//Extendido:
for(Integer num : numbers){
    System.out.println(num);
}

//Java 8:
numbers.forEach(x -> System.out.println(x));

//Too cool:
numbers.forEach(System.out::println);
```

 - **Double colon operator**
 
  `.map(i -> Tests.doubleIt(i))`
  
  it can be changed to
  
  `.map(Tests::doubleIt)`
  
 You can also refer to instance methods using someObject::someMethod, or even to constructors using SomeClass::new.
  

